{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sophia HPC cluster","text":"<p>This website is a resource with technical details for users to make efficient use of the computing and storage resources of the Technical University of Denmark's high performance computing cluster Sophia.</p> <p>The Sophia HPC cluster runs the Linux CentOS operating system and the cluster installation is based on the OpenHPC framework.</p> <p>The default shell is bash and users are encouraged to use bash  since Sophia's compute job scheduler primarily supports this shell.</p>"},{"location":"#acknowledgment-of-use","title":"Acknowledgment of use","text":"<p>Please use DOI 10.57940/FAFC-6M81 in your publication; select citation format here https://doi.datacite.org/dois/10.57940%2Ffafc-6m81, and e.g. use the following text as appropriate:</p> <p>The authors gratefully acknowledge the computational and data resources provided on the Sophia HPC Cluster at the Technical University of Denmark, DOI: 10.57940/FAFC-6M81.</p>"},{"location":"#guidelines-for-use","title":"Guidelines for use","text":"<p>Through constructive dialogue we wish on one hand to ensure continuity and progress in on-going research activities and on the other hand to promote the general use of HPC in research at DTU and other Danish universities.</p> <p>Optimal use of the HPC resources available requires responsible and conscientious use from the individual user. </p> <ol> <li>Read these docs pages! </li> <li>When in doubt, ask! Write the address at the bottom left.</li> <li>The HPC resources can be used for development and testing of applications; please ensure that reserved HPC resources are utilised and not standing idle. E.g. do not reserve multiple nodes for code that only utilise resources on a single node.</li> <li>Carefully consider the impact of reserving several nodes on other users' ability to carry out their work; e.g. inspect current load prior to job submission, e.g. postpone resource-demanding compute jobs until evening, etc.</li> <li>Please strive to reserve and utilise full compute nodes to the extent feasible.</li> <li>Do not run code on the login node! Request an interactive job.</li> <li>Jobs or login node usage that negatively impacts general use will be stopped and access for the offending user may be restricted or denied if this occurs repeatedly. The decision of restriction of access or closing an account is taken in consensus between the DTU IT Department, DTU Wind Energi and DTU Mechanical Engineering, each having one representative.</li> </ol>"},{"location":"access/","title":"How to access HPC resources","text":"<p>DTU Sophia is accessed via the Secure Shell protocol  and hence a ssh client is needed on the user's laptop from which connection is to be established to DTU Sophia. This is preferably installed via the laptop operating system's software package manager.</p>"},{"location":"access/#when-outside-dtu-network","title":"When outside DTU network","text":""},{"location":"access/#dtu-employee","title":"DTU Employee","text":"<p>Use your laptop's ssh client from a terminal or create a  virtual private network (VPN) connection.</p> Tool URL Example ssh client; e.g. OpenSSH ssh.risoe.dk <code>ssh my-dtu-username@ssh.risoe.dk</code> vpn client; e.g. OpenConnect (Linux) or AnyConnect (Windows) vpn.dtu.dk <code>openconnect --os=win vpn.dtu.dk</code>"},{"location":"access/#deic-grant-holder","title":"DeiC grant holder","text":"<p>Log on the remote portal and choose DeiC Sophia Desktop  - this will open a Citrix session with Linux CentOS 7.</p>"},{"location":"access/#accessing-sophia-login-node","title":"Accessing Sophia login node","text":""},{"location":"access/#linux","title":"Linux","text":"<p>Open a terminal and access Sophia from the commandline;</p> <pre><code>ssh &lt;username&gt;@sophia.dtu.dk\n</code></pre>"},{"location":"access/#microsoft-windows","title":"Microsoft Windows","text":"<p>The MobaXterm Windows application offers a range of features, including e.g. default graphics forwarding. Use the same command as for  Linux.</p>"},{"location":"account/","title":"Getting a user account","text":"<p>In order to access HPC resources a user account is necessary.</p>"},{"location":"account/#dtu-employee","title":"DTU employee","text":"<p>Students must have a DTU staff member (e.g. supervisor) forward user account  requests on their behalf in order to access DTU Sophia resources.</p> <p>DTU staff must direct account creation requests to the address at the bottom left.</p>"},{"location":"account/#deic-grant-holder","title":"DeiC grant holder","text":"<p>A DeiC grant holder can request access to DTU Sophia resources by  forwarding - to the address at the bottom left - correspondence with the relevant user  creation approver.</p> <p>Until more elaborate guidelines are finalised,  Dan Ariel S\u00f8ndergaard is acting approver for DTU Sophia resource requests from employees at other Danish universities.</p>"},{"location":"hardware/","title":"DTU Sophia hardware","text":""},{"location":"hardware/#compute-nodes","title":"Compute nodes","text":"<p>The Sophia HPC cluster consists of 516 computational nodes of which 484 are 128 GB RAM nodes and 32 are 256 GB RAM nodes. Each node is a powerful x86-64 computer, equipped with 32 physical cores (2 x sixteen-core AMD EPYC 7351).</p> <p>The parameters are summarized in the following table:</p> Specs Primary purpose High Performance Computing Architecture of compute nodes x86-64 Operating system CentOS 7 Linux Compute nodes in total 516 Processor 2 x AMD EPYC 7351, 2.9 GHz, 16 cores RAM (484 nodes) 128 GB, 4 GB per core, DDR4@2666 MHz RAM (32 nodes) 256 GB, 8 GB per core, DDR4@2666 MHz Local disk drive no Compute network / Topology InfiniBand EDR / Fat tree In total Total theoretical peak performance  (Rpeak) ~384 TFLOPS (516 nodes x 32 cores x 2.9GHz x 8 FLOP/cycle) Total amount of RAM 69 TB"},{"location":"hardware/#high-speed-interconnect","title":"High-speed interconnect","text":"<p>The nodes are interlinked by InfiniBand and 10 Gbps Ethernet networks.</p> <p>Sophia's high-speed, low-latency interconnect is Mellanox EDR (100Gbps) Infiniband. Frontend-, compute-, and burst buffer nodes each have Mellanox' ConnectX-5 adapter card installed.</p> Switch system Count SB7700 2 SB7790 47"},{"location":"hardware/#burst-buffer","title":"Burst buffer","text":"Hardware product Count Dell R7425 w/NVMe front-bay 2 Dell Express Flash PM1725a 1.6TB 16 Dell Express Flash PM1725b 1.6TB 4"},{"location":"modules/","title":"Software modules","text":"<p>List available modules</p> <pre><code>ml avail\n</code></pre> <p>and load a particular module, e.g.</p> <pre><code>ml OpenMPI/3.1.1-GCC-7.3.0-2.30\n</code></pre> <p>to use OpenMPI version 3.1.1 compiled with GNU Compiler Collection version 7.3. To get an overview of modules currently loaded in the shell environment do</p> <pre><code>ml\n</code></pre> <p>Unload a particular module with <code>ml unload &lt;module&gt;</code>, e.g.</p> <pre><code>ml unload GCC/7.3.0-2.30\n</code></pre> <p>or just <code>ml unload GCC</code> will remove the GNU Compiler Collection from the environment (and OpenMPI would stop functioning!).</p>"},{"location":"modules/#installing-software","title":"Installing software","text":"<p>As an alternative to building software manually a HPC package manager can be used.</p>"},{"location":"modules/#easybuild","title":"EasyBuild","text":"<p>Sophia users can load the EasyBuild framework with</p> <pre><code>ml EasyBuild\n</code></pre> <p>and e.g. go through a workflow example to get the hang of it. An alphabetically sorted list of software recipies - so-called easyconfigs - are available on GitHub.</p>"},{"location":"modules/#spack","title":"Spack","text":"<p>Another option is the Spack package manager; we refer to the docs for further information.</p>"},{"location":"permanent/","title":"Long term storage","text":""},{"location":"permanent/#ceph-file-system","title":"Ceph file system","text":"<p>The file systems mounted under <code>/home</code> and <code>/groups</code> are ceph file systems. Your home folder is for configuration files and personal data, for example, ssh-keys, local copies of code for development, publication drafts and referee reports. Never store any data intended for sharing in your home folder.</p> <p>Group folders are for data shared within a group of researchers, for example, members of a department or section, members of a project or users of a licensed or otherwise restricted software. Group folders should be used for storing all research data, including data resulting from master-, Ph.D.- and post-doc projects. Supervisors of such projects should ask for creation of a group folder for this purpose.</p> <p>Furthermore, these two file systems are intended for storing warm and cold data but not hot or temporary data:</p> <ul> <li>Cold data (archival data): Data with a long life-time (years to decades to forever). This data is typically immutable and non-reproducible.</li> <li>Warm data (work in progress data): Data with intermediate life-time (days to weeks to months). Some of this data will become cold data, others will be deleted at the end of a project or after publication.</li> <li>Hot data (job working set): Data with a life-time of a job execution. This is all data that is required during a run of a job, but has no purpose after job completion.</li> </ul> <p>Never save hot data on the ceph file systems! This produces unnecessary load on the shared file systems and will pollute the ceph snapshots. Please use the burst buffer or the local RAM disk for hot and temporary data.</p>"},{"location":"permanent/#performance-considerations","title":"Performance considerations","text":"<p>The ceph filesystem is optimized for capacity, data protection, cost and sequential throughput - in this order. In addition, ceph is not a parallel file system like, for example, Lustre or BeeGFS. This has important implications on performance and how to run jobs using the various file systems in the best possible way.</p>"},{"location":"permanent/#ideal-work-flow","title":"Ideal work flow","text":"<p>The ceph file system is designed for best performance with a temporary scratch workflow:</p> <ol> <li>Copy multi-access data to temporary storage (burst buffer or RAM disk). Ideally, this is an extraction from a single large archive (tar, hdf5, zip, ...) directly to temporary storage, which will benefit from the high streaming bandwidth of ceph.</li> <li>Data accessed only once by an application should be directly read from ceph. Ideally, access is sequential from large files.</li> <li>During job execution, all new data is produced on temporary storage. Use the burst buffer if parallel multi-node access is required, or the RAM disk for single-node access. RAM disk is the fastest option but provides also the smallest amount of temporary storage capacity.</li> <li>At the end, copy all permanent data back to ceph. Ideally, this is a creation of an archive (tar, hdf5, zip, ...) directly to ceph, which will again benefit from the high streaming bandwidth of ceph.</li> </ol> <p>You can use a simplified workflow if your application</p> <ol> <li>sequentially reads some (large) files only once,</li> <li>does not require any disk access during computation and</li> <li>writes some (large) files sequentially after completion.</li> </ol> <p>Here, 'some' is a small number, like 5-10 files but not hundreds or thousands. Applications with this access pattern should run directly on the ceph file systems.</p>"},{"location":"permanent/#performance-characteristics-dos-and-donts","title":"Performance characteristics, Do's and Dont's","text":"<p>Our ceph file systems</p> <ul> <li>are optimized for large-file I/O. Creating, copying, deleting, listing large numbers of small files is slow. Ideally, use archival data formats like tar or hdf5 on ceph.</li> <li>provide ca. 2000 aggregated IOP/s (random 4K writes) and an aggregated bandwidth of ca. 4.7GB/s (sequential 1M writes).</li> <li>reach link speed (ca. 850MB/s) with single node sequential 1M writes.</li> </ul> <p>Differently to the Lustre file system used on Sophia predecessor systems, the ceph file system is not a parallel file system. Furthermore, the ceph file system is not a low-latency storage system either, the latency is of the order of 1ms. The ceph file system is designed for cloud applications, where every client (VM, user, app) has exclusive access to its own directory sub-tree. In other words, no two clients access files in the same directory sub-tree.</p> <p>While ceph supports multi-client (multi-node) access to files in the same directory sub-tree, this access is not necessarily concurrent as with parallel file systems. Rather, the ceph storage cluster serializes concurrent access whenever necessary to present consistent data and meta-data information across all clients. In addition, ceph maintains cache coherency between clients, meaning that a write on one node can invalidate read cache on another node.</p> <p>These characteristics can result in an counter-intuitive experience when transitioning from a parallel file system to a ceph file system. Most of the pitfalls can be avoided by following our ideal workflow outlined above. We hope this \"Do's and Dont's\" list helps avoiding others:</p> <ul> <li>Do check this best practices guide for good and bad workloads.</li> <li>Don't use multi-node concurrent file access. Do collection of data on the job's master node and write data to disk only from the master node.</li> <li>Don't send a ticket just because this <code>ls -l</code> doesn't finish in 5s. A slow response is almost certainly not indicating a malfunction of the ceph cluster, but rather a bad workload like concurrent access of a directory sub-tree.</li> <li>Don't run a job and check its progress with <code>ls -l</code> on the head node. This creates a concurrent access with all the synchronization overhead. It will not only be slow on the head node, it will also stall the job. Do ssh/mrsh into the master node of the job and run your <code>ls -l</code> there (i.e. use single-node access).</li> <li>Don't submit a sequence of jobs using the same application. After each job the SLURM prologue script flushes the file system cache on the compute nodes. To avoid reloading the same data from disk for every computation, do run a sequence of computations with the same application in one job. This is particularly important for applications that open thousands of files, like Matlab. Use <code>srun</code> to execute single-application multiple-input jobs in an efficient way.</li> <li>Don't call <code>fflush</code> and <code>fsync</code>, <code>sync</code> excessively, for example, after every single write. Let writes accummulate in the system buffer and call one of these functions after at least 4MB worth of writes are processed.</li> <li>Do use <code>fclose()</code> after writes are completed to ensure data is committed.</li> </ul>"},{"location":"permanent/#snapshots","title":"Snapshots","text":"<p>In case of accidental deletion, corruption or modification of data by malware, previous versions are available in ceph snapshots. To access a snapshot of a file, change to the file's directory and execute <code>cd .snap</code>. Note that \".snap\" is a meta-directory, not a real directory. Therefore, tab-completion and the like will not work and one needs to type it explicitly as stated here. Inside the snapshot directory are folders with current snapshots, for example,</p> <pre><code>[frans@sophia1 .snap]$ ls\n_2021-02-23_183554+0100_weekly_1099511719926  _2021-02-27_000611+0100_daily_1099511719926\n_2021-02-24_000611+0100_daily_1099511719926   _2021-02-28_000611+0100_daily_1099511719926\n_2021-02-25_000611+0100_daily_1099511719926   _2021-03-01_000611+0100_daily_1099511719926\n_2021-02-26_000611+0100_daily_1099511719926   _2021-03-01_000911+0100_weekly_1099511719926\n</code></pre> <p>To access snapshots of a specific date, cd into the corresponding folder and browse through the files. Note that files in snapshots are read-only regardless of the file permissions shown by <code>ls -l</code>.</p> <p>Disclaimer</p> <p>Snapshots are provided as a courtesy service on the ceph file systems <code>/home</code> and <code>/groups</code>. The availability of snapshots depends on the amount of currently unused ceph storage capacity and the retention time may be reduced without warning to adjust to increased utilisation. The default retention scheme is:</p> <ul> <li>daily snapshots for 7 days and</li> <li>weekly snapshots for 4 weeks.</li> </ul> <p>We cannot guarantee snapshots to be present in situations where excessive amounts of temporary data have been captured in snapshots. We aim, however, to have at least the last two days available.</p>"},{"location":"permanent/#advanced-topics","title":"Advanced topics","text":"<ul> <li>LazyIO provided by libcephfs</li> <li>Ceph fs and POSIX.</li> </ul>"},{"location":"scheduler/","title":"The Slurm job scheduler","text":"<p>Sophia uses the Slurm cluster management and job scheduling system. Computations on Sophia are executed through interactive Slurm sessions or via batch compute job scripts.</p>"},{"location":"scheduler/#job-queues-aka-partitions","title":"Job queues a.k.a. partitions","text":"<p>Usage of particular computation resources available on Sophia is governed by specification of a job queue, or partition in Slurm lingo, which can be done either on the commandline as argument to the <code>srun</code> command or via a batch job script that is submitted to the scheduler with the <code>sbatch</code> command.</p> <p>Sophia compute nodes are organised in the following job queues/partitions:</p> Name Open for all Sophia users workq AMD EPYC 7351 (1st gen, 32 cores), 128 GB memory rome AMD EPYC 7302 (2nd gen, 32 cores), 128 GB memory fatq AMD EPYC 7351, 256 GB memory gpuq 1 Nvidia Quadro P4000 GPU per node v100 1 Nvidia Tesla V100 per node Name Exclusive access for DTU Wind Energy staff windq AMD EPYC 7351 (1st gen, 32 cores), 128 GB memory windfatq AMD EPYC 7351 (1st gen, 32 cores), 256 GB memory <p>Use the <code>sinfo</code> command to list information about the Slurm partitions configured and <code>squeue</code>  to list compute jobs on queue.</p>"},{"location":"scheduler/#interactive-jobs","title":"Interactive jobs","text":"<p>For code testing purposes an interactive terminal session is convenient and can be requested using the <code>srun</code> command. The following example illustrates the procedure;</p> <pre><code>srun --partition windq --time 06:00:00 --nodes 2 --ntasks-per-node 32 --pty bash\n</code></pre> <p>which requests an interactive job with 2 compute nodes from the <code>windq</code> partition, and all 32 physical cores available on each, for 6 hours. The session is granted when resources become available and ends when the user exits the terminal or once the time limit - here 6 hours - is reached.</p>"},{"location":"scheduler/#batch-jobs","title":"Batch jobs","text":"<p>Once the simulation workflow has been tested via interactive jobs one may wish to run a number of jobs unsupervised. To dispatch jobs programmatically a job script must be prepared;</p> <p>Slurm job script example</p> <p><code>[&lt;username&gt;@sophia1 ~]$ cat slurm.job</code></p> <pre><code>#!/bin/bash\n#SBATCH --time=2:00:00\n#SBATCH --partition=workq\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\necho \"hello, Sophia!\"\n</code></pre> <p>which can then be submitted with the <code>sbatch</code> command,</p> <pre><code>[&lt;username&gt;@sophia1 ~]$ sbatch slurm.job\n</code></pre>"},{"location":"scratch/","title":"Temporary storage performance-hierarchy","text":"<p>The temporary storage options are listed below.</p>"},{"location":"scratch/#each-compute-nodes-local-tmp-directory","title":"Each compute node's local <code>/tmp</code> directory","text":"<p>Sophia compute nodes are ephemeral meaning they fetch a minimal CentOS installation at each reboot which then runs in the node's memory. This means that the underlying XFS file system runs on the compute node's random access memory hardware with orders of magnitude higher bandwidth and lower latency compared with disk drives. Thus, e.g. to eliminate I/O as the limiting factor in a application performance benchmark experiment, a Sophia user can run test code from the <code>/tmp</code> directory on up to 32 MPI processes on a single Sophia node with 32 physical cores.</p> <p>The RAM-based storage provides approximately 64GB capacity per node (minus OS usage).</p>"},{"location":"scratch/#each-compute-nodes-local-scratch-directory","title":"Each compute node's local <code>/scratch</code> directory","text":"<p>Each compute node is equipped with a 1TB spinning disk mounted at <code>/scratch</code>. This provides significantly more storage space than the RAM-based <code>/tmp</code> directory, while still offering good I/O performance for local storage needs. This storage is node-local and is ideal for larger temporary datasets that will not fit in RAM but do not require shared access across nodes.</p>"},{"location":"scratch/#burst-buffer","title":"Burst buffer","text":"<p>Two servers with NVMe disks - Sophia's burst buffer nodes - are connected to Sophia compute nodes via the HPC cluster's Mellanox EDR Infiniband interconnect, and each server connects to the Ceph file system via bonded 10Gbps connections (i.e. 20Gbps). Sophia's burst buffer runs the BeeGFS file system, striping data written from  compute nodes across BeeGFS storage targets. The storage resource is mounted on <code>/work</code> on Sophia's head node and all compute nodes and  provides approximately 50TB of shared storage space accessible from all nodes.</p>"},{"location":"scratch/#automatic-temporary-directory-management","title":"Automatic Temporary Directory Management","text":"<p>To simplify temporary storage usage and ensure proper cleanup, we have implemented automatic temporary directory management through predefined SLURM variables:</p> <ul> <li><code>$TMPRAM</code> - Points to <code>/tmp/users/$SLURM_JOB_UID/$SLURM_JOB_ID</code></li> <li><code>$TMPDISK</code> - Points to <code>/scratch/users/$SLURM_JOB_UID/$SLURM_JOB_ID</code></li> <li><code>$TMPSHARE</code> - Points to <code>/work/users/$SLURM_JOB_UID/$SLURM_JOB_ID</code></li> </ul> <p>These directories are automatically created when your job starts and deleted when your job completes, regardless of whether the job completes successfully or fails. This eliminates the need for manual cleanup and helps maintain system performance.</p>"},{"location":"scratch/#usage-example","title":"Usage Example","text":"<pre><code>#!/bin/bash\n#SBATCH [your job parameters]\n\n# Use RAM-based storage for very fast I/O\n./fast_io_program --workdir=$TMPRAM\n\n# Use local disk for larger datasets\ncp large_input.dat $TMPDISK/\n./analysis_program --input=$TMPDISK/large_input.dat --output=$TMPDISK/results.dat\ncp $TMPDISK/results.dat $HOME/results/\n\n# Use shared storage for multi-node access\nmpirun -n 64 ./parallel_sim --output=$TMPSHARE/simulation_output/\n</code></pre> <p>Important: Any data you wish to keep must be copied to your home directory or permanent storage before your job ends, as the temporary directories will be automatically removed.</p>"},{"location":"sharing/","title":"Private and shared","text":"<p>On the Sophia storage system there are two types of directories; subdirectories for single-user data are under <code>/home</code>, whereas subdirectories for multi-user data are  under <code>/groups</code>.</p> private data shared data <code>/home/&lt;dtu user&gt;</code> <code>/groups/&lt;group name&gt;</code> <p>Multi-user directories must never contain personal user data (ssh keys, etc.).</p> <p>If you would like to have a group directory created please write the address at the bottom left.</p>"},{"location":"subsystems/","title":"Sophia cluster systems","text":"<p>The Sophia HPC cluster consists of</p> <ul> <li>login- and compute nodes</li> <li>burst buffer nodes for temporary data</li> </ul> <p>each node with fast interconnect fabric.</p> <p>Embedded in the racks, alongside Sophia compute- and burst buffer nodes, is a</p> <ul> <li>storage cluster for permanent data</li> </ul> <p>hosting the <code>$HOME</code> directory for Sophia users as well as shared directory infrastructure to facilitate collaboration on projects. The storage cluster is connected to compute nodes and burst buffer with high-end commodity interconnect fabric.</p>"},{"location":"transfer/","title":"Data transfer","text":"<p>In this section methods for moving data to and from Sophia are described.</p>"},{"location":"transfer/#using-sshfs","title":"Using sshfs","text":"<p>Secure SHell FileSystem (SSHFS) allows to mount remote locations on a local machine. Any user storage resources reachable with <code>ssh</code> can be mounted as a file system.  Since SSHFS creates a user space file system, administrator privileges are not required.</p>"},{"location":"transfer/#sshfs-on-linux-laptop","title":"sshfs on Linux laptop","text":"<p>Install via your package manager or build from source. Then identify Linux id for your laptop user and primary group</p> <pre><code>$ id $USER\n</code></pre> <p>which are the <code>uid</code> and <code>gid</code> numbers in the command's output.</p> <p>On your laptop, create a mount path owned by your local user, e.g.</p> <pre><code>sudo mkdir -p /mnt/sophia/$USER\nsudo chown $USER.$USER /mnt/sophia/$USER\n</code></pre> <p>and e.g. mount your Sophia <code>$HOME</code> path with</p> <pre><code>sshfs -o uid=&lt;local user uid&gt;,gid=&lt;local user gid&gt; &lt;dtu user&gt;@sophia.dtu.dk:/home/&lt;dtu user&gt; /mnt/sophia/&lt;local user name&gt;\n</code></pre>"},{"location":"transfer/#sshfs-on-windows-laptop","title":"sshfs on Windows laptop","text":"<p>There are a number of options, e.g.</p> <ul> <li>Install latest WinSFP from github</li> <li>Install latest SSHFS-Win from github</li> <li>Open File Explorer, right-click on <code>This PC</code> and choose <code>Map network drive</code>. Choose a drive to mount at and in the Folder field enter</li> </ul> <pre><code>\\\\sshfs\\&lt;dtu user&gt;@sophia.dtu.dk\n</code></pre> <p>to mount your Sophia $HOME directory.</p>"},{"location":"transfer/#using-sftp","title":"Using sftp","text":""},{"location":"transfer/#filezilla","title":"FileZilla","text":"<p>This is probably one of the easiest and fastest way to transfer files. FileZilla allows for recursive file transfer, concurrent file transfers, uses incremental file transfer and allows to resume interrupted file transfers.</p> <p>FileZilla is a file transfer client, which runs under Windows, Linux, Mac/IOS and other systems. It is able to transfer files via sftp and ssh file transfer protocols. For most Linux distributions pre-compiled packages are available. The HPC clusters support the ssh file transfer protocol. The set-up is very simple:</p> <p>Commands are shown for the Ubuntu operating system and connecting to Jess.</p> <ol> <li>Install FileZilla.</li> <li>Start FileZilla and open the site manager under File-&gt;Site Manager (or press +s). <li>Create a new site with name Sophia.</li> <li>In the general tab, enter:<ol> <li>Host: <code>sophia.dtu.dk</code></li> <li>Protocol: SFTP - SSH File Transfer protocol</li> <li>Logon Type: Ask for Password</li> <li>User: <code>&lt;dtu user&gt;</code></li> </ol> </li> <li>Click <code>connect</code> to test the connection.</li> <p>A particularly nice feature of FileZilla is synchronized browsing, which is useful for keeping entire directory trees in sync. It also allows editing of remote files from the local computer.</p>"}]}